>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: Group Number      |
|-----------------|
| Student 1: Sufyan Ayaz                |   
| Student 2: Muhammad Haris Kashif              |   
| Student 3: Faisal Islam               |   
| Student 4: Taha Khan                |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#user-content-introduction)

[2 High-level description of the exploratory testing plan	1](#user-content-high-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing	1](#user-content-comparison-of-exploratory-and-manual-functional-testing)

[4 Notes and discussion of the peer reviews of defect reports	1](#user-content-notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5 How the pair testing was managed and team work/effort was
divided	1](#user-content-how-the-pair-testing-was-managed-and-team-work/effort-was)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#user-content-difficulties-encountered,-challenges-overcome,-and-lessons)

[7 Comments/feedback on the lab and lab document itself	1](#user-content-comments/feedback-on-the-lab-and-lab-document-itself)

# Introduction

In this lab, we were given the opportunity to employ real life testing methodologies as a way to introduce ourselves to software testing, and to gain a comprehensive understanding of the foundational principles that would allow us to succeed in the realm of testing. We were given access to two versions of an ATM application (an initial version and an updated version) and were given the task of isolating potential bugs that the application may have. The process of debugging the application involved three different types of software testing techniques: exploratory testing, manual functional testing, and regression testing. Before beginning this lab, our group had a prior familiarity with two of the techniques: exploratory testing and manual functional testing. With regards to exploratory testing, this was the testing technique that our team first employed after reading over the application description. The members of our team had not ever attempted to debug an application using this technique (prior to this lab), but we were all familiar with the workings of the technique and how it involved finding bugs through the use of ones cognitive abilities and understanding of the application’s design. After exploratory testing, the next testing technique we employed was the manual functional testing technique. When it comes to manual functional testing, our team had the most knowledge of this testing method having practically experienced it in the past. We understood that the technique involved verifying the basic functionality of the application using a set of predetermined test cases, and because of our previous experience with this method, this was the testing technique that was easiest for us to grasp and execute. Finally, we employed the regression testing technique, where our team had re-ran test cases in the updated version of the ATM application to monitor the resolution status of old bugs, or the appearance of new bugs. The members of our team had no prior experience or knowledge of this testing technique, but it was relatively easy to understand, so we didn’t have issues with this technique.

# High-level description of the exploratory testing plan

Prior to beginning our exploratory testing we needed to make sure we were well informed about the application and its features. This meant we needed to read over the lab document multiple times, discuss features that were listed in the application, and take notes on feature specific details, such as what happens on incorrect pin inputs. After fully informing ourselves on the application and its features, we planned how we would test the application. After a few discussions we decided to first freely explore the application and familiarize ourselves with all the features and their corresponding UI. While freely exploring the application we were actively searching for anything out of the ordinary, such as spelling mistakes, incorrect receipts, or any differences between the provided documentation and the actual application. Through this portion of our exploratory testing plan, we would stumble upon a few bugs, and begin documenting them. After about an hour of running through the application the intended way, we switched over to our second portion of exploratory testing, which was where we looked at testing the application in unintended ways. This meant not following the flow of the application but rather attempting to actively break it. After about another hour of testing, many more software breaking bugs were found, specifically bugs that completely froze the application and some infinite money glitches. Overall our exploratory testing was split into three separate parts, firstly, familiarizing ourselves with the documentation provided and the expected behavior, secondly, running through the application the intended way, and lastly, running through the application the unintended way.


# Comparison of exploratory and manual functional testing

Exploratory testing and manual functional testing were two of the testing techniques that our team employed during the testing of the ATM application. Both testing methods have their own respective benefits, but they do differ in terms of their execution and what they are able to achieve.

Exploratory testing is a lot more flexible than manual functional testing. It’s more of a trial and error type testing technique that really relies on the tester being familiar with how the application is designed to work, so they can try to find bugs on the path that a normal user would take. This testing technique takes away from the pressure of ensuring that the application is meeting a certain standard at every step and really just allows the tester to learn about and experience the application as a user, which can lead to them finding the random, edge-case bugs that they wouldn’t normally be able to find if they were sticking to a plan. This testing also allows for debugging of newer systems to become easier as it doesn’t require the tester to know everything about the application. Even though exploratory testing may seem like a very effective testing method, it does have drawbacks that can take away from its effectiveness. The most significant drawback is that exploratory testing is very time consuming. Not knowing what bugs they are trying to find can lead to testers spending a long time repeating the same actions and not finding new bugs, which can be quite detrimental if there are deadlines during the testing phase of the application. Also, the effectiveness of exploratory testing quite heavily relies on the ability of the tester themselves. If the tester is not able to properly acquaint themselves with the application design, is not very experienced, or is not very creative, they can very easily miss a lot of bugs and render the testing as being ineffective. Another drawback that exploratory testing faces is that this technique does not work very well when it comes to large applications. Our ATM application was quite small, which made exploratory testing feasible, but exploratory testing on large application would not yield a similar success rate. The lack of a plan when testing a large system can make it very hard to find all the bugs, and is most likely going to result in many bugs being overlooked. A third drawback that arises when using exploratory testing is that sometimes, when testers stumble upon random edge-case bugs, it can be difficult to reproduce the same bug as those bugs are found in unexpected places.

In contrast to exploratory testing, manual functional testing is all about testing applications in a rigid, systematic fashion. This method involves having a detailed testing plan, in the form of test cases in a testing suite, that will cover all the important functionalities of the application, and will ensure that they are up to standard. Having a plan of action allows for this method to be much more faster and effective when finding bugs in application, as testers already have a sense of what they are looking for and can directly go there. Since there is a plan involved for finding the bugs, it is also much more easier to reproduce bugs as testers would have the steps they took to find the bug in the first place. The manual functional testing method is also useful when it comes to testing large application, as time isn’t wasted and testers can be guided through the app through their test suite. Our ATM application was quite small, so a lot of the bugs were found during the exploratory testing phase, but when we were in the manual functional testing phase, we were able to find remaining bugs much faster as we now knew what to look for. Even though manual functional testing seems like quite a promising testing technique, it does have its own drawbacks as well. For starters, because of how systematic and planned out this testing technique is, it is very easy for testers to follow the plan to test that all the functionalities are working properly and overlook bugs that may exist within edge-cases as those cases are outside the planned tests (which is something we noticed when we were testing our ATM application and found that there were quite an amount of bugs that we found in the exploratory testing phase that weren’t covered by the test cases in the testing suite we were given). Another drawback that manual functional testing has is that because of how comprehensive it is, in order to predetermine such systematic test cases, the people coming up with the test cases need to have a very thorough and complete understanding of the application and all of its functionalities.

Both testing types have their own benefits and drawbacks, but what our team understood during this lab assignment was that the reason they both work very well together (and why they should both be used when testing software) is because of how they compliment each other. Both testing styles make up for the others drawbacks and together allow for a more comprehensive debugging/testing of applications.


# Notes and discussion of the peer reviews of defect reports

Once each pair was done with testing, we had two defect reports, one for each pair. Reviewing each other's reports, we saw that there were a few instances of the same bugs being found. Because both pair's testing process for exploratory testing was to go through the ATM and try all the features out in an attempt to find any bugs, some of the major bugs were the same. Most of the bugs that both pairs had found were bugs that were easy to notice. A few examples of this were the Deposit, withdraw, and transfer bugs, where the amount of money that was being deposited, withdrawn, or transferred was different from what the user had requested. However, because two pairs were testing, there were a few bugs in both reports that the other pair had not noticed while testing. We noticed that a lot of the bugs that only one group had found were bugs that were hard to notice. Examples of these bugs would be during transfer, where the names of the accounts were reversed on the receipt or when the card number on the receipt of a transaction was different from what the customer was using. To find these types of bugs, we had to look between the lines, and these were the bugs that one pair or the other missed out during testing. We also decided to do the manual scripted testing in pairs as well, where one pair did tests 1-20 and the other pair did tests 21-40. Each pair included these tests in their defect reports. We noticed that some of the bugs that were found during manual scripted testing were already found during exploratory testing. Since all the 40 test cases were different for manual scripted testing, there weren't similarities in the report for that portion. The overall format of both reports was quite similar which made it easy to compare and combine at the end, as neither pair had significantly more information than the other. All in all, two reports made it easier for us to find bugs in the application, especially since it's easy to miss small details, whereas having two pairs helped catch those deatails and create a more detailed report.

# How the pair testing was managed and team work/effort was divided 

Our team utilized Discord for communication. We split our team into two pairs: 
* Pair 1: Faisal Islam & Taha Khan
* Pair 2: Sufyan Ayaz & Muhammad Haris Kashif

For our exploratory testing, we tested different features of the application, in both versions. The way we divided this up was each pair sat with their respective partners and “played around” with the program and tested functionality to attempt to “break” the program and find bugs. Each partner tested a different version of the application, so one partner tested v1.0, and the other tested v1.1. We felt the most efficient way to cover our bases and ensure that we kept track of patched bugs and unpatched bugs between versions of the application was for each partner to replicate a bug on the version they were testing.  

For our manual scripted testing, Pair 1 tested cases 1-20, and Pair 2 tested cases 21-40. Furthermore, each pair worked on 10 test cases each, dividing the work up evenly among the four members of the team. We each regression tested our assigned cases in both versions. After testing and reporting the findings of our respective test cases, we reconvened with our pairs to kind of cross check and establish that there were minimal errors in our testing methodologies. 

Each pair and the individuals in each pair logged the bugs discovered within their exploratory and manual scripted testing. After we fully completed our testing, we reviewed the application once more as a group in an attempt to find more bugs we may have initially overlooked, and also to ensure that we completed our test cases correctly. 

# Difficulties encountered, challenges overcome, and lessons learned

As we were playing around with the system and searching for bugs one of the main issues we encountered was determining the difference between bugs and features. Oftentimes there were features found but did not have any documentation, so determining if it was intentional or not proved challenging. An example of this can be seen when users deposit money into the system, in version 1.0, ten dollars were lost every time the user would make it deposit, at first it would appear to be a bug, however in version 1.1, ten cents would be removed. Oftentimes ATM machines do have transaction fees, and it proved difficult to determine if the ten cents are considered to be a bug or a feature. However after careful consideration and thorough reading of the documentation, we decided on reporting it as a bug. This was not the only challenge we faced, as another issue arises, after peer reviewing each other's bugs. Some bugs found on version 1.1, would only occur on the windows version of the application, and not the MAC version. For example, there is a bug in version 1.1 where if you press one number higher on the keypad, then the provided options, 20 dollars would be dispensed from the ATM. This bug was reproducible on windows, but not on mac. This made documentation of bugs much harder as different bugs would occur on different OS systems. Throughout all of these challenges we faced, there were a few lessons we had learned. The first being that both exploratory and manual functional testing are needed, when testing an application. Originally we were worried we would find all the bugs in the exploratory phase and have none left for the MFT phase. However we were quickly proven wrong, although there was some overlap in bugs found, there was still an abundance of new bugs found in the MFT phase. This helped us realize the importance of both exploratory testing and manual functional testing. Lastly, another lesson we learned was the importance of testing software on multiple OS’s. As we saw during our own testing just cause the application’s are meant to be the same, does not mean that issues will not arise. Thus teaching us that it is vital to test all platforms an application is being shipped to.

# Comments/feedback on the lab and lab document itself

Sufyan: Overall, I felt this lab was quite beneficial for me. Being able to implement the three testing techniques together allowed for me to get a practical understanding of the basics of how software is tested in real life. This lab also gave me an insight into why these three testing techniques are used together and individually as well. One issue I did have with the lab is that I felt the instructions could have been a bit clearer with more comprehensive steps, as I found at times it was difficult to follow the instructions.

Haris: I think this lab was a good experience overall. While I have done testing before, it was my first time creating a defect report for the issues that I found, and that made it a good learning experience for me. It also allowed me to learn more about exploratory, manual scripting and regression testing and taught me why they can be useful methods of testing. The lab document itself was fine for the most part, however, I feel it could've been more detailed in telling us what we needed to do, as it took me longer than it should've to understand what our task was.

Taha: Overall I think the Lab was very fun. It was nice to just play around with software and look for as many bugs as possible. It was nice to be testing an application rather than designing it. The Lab also provided me with the basics of testing and did so in a fun and engaging way. However, although I thoroughly enjoyed the lab, I think the lab document has some areas in which it could improve. It would be nice if there could have been more information on the rules of the system and how the system actually works. Although there was quite a bit of documentation provided, it still missed some key details, like if there was a transaction fee, that would prove helpful when determining the difference between a bug and a feature. Overall the lab was very fun and enjoyable, with some small improvement that could have been made to the lab document itself.

Faisal: Overall I enjoyed the lab and I believe it was beneficial in learning about and introucing the concept of formal testing. I have done testing before using JUnit and just overall testing applications and frontends prior to deployment, but doing it in this structured method and logging a bug report is something new and I definitely can see the merit and the benefits of doing structured formal testing in this manner, so we can ensure total full coverage testing that touches all bases and aspects of an application.  The only issue I really encountered was the instructions/document and the format it was presented in, where I feel steps, outcomes, and expectations could have been made more clear and put into a more readable format like a PDF. Reading in the markdown format in GitHub is a bit of a challenge, at least for me, but this may just be a personal pet peeve. 

